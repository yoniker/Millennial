---
layout: post
title: "Show Cases"
author: "Yoni Keren"
categories: documentation
tags: [documentation]
image:
  feature: classification_animation2.gif
---

Here are a few examples showcasing the way Baby Step works.
The input to the classifier is an image, and the output is an image as well, bounding boxes around faces in the picture with each person's classification on the top of their bounding box.
This is exactly what you are going to see when running the net+python scripts I've provided.

Notice that unlike many (most?) Machine Learning articles/projects, I've elected to showcase bad errors as well.
In fact, I've chosen some random pictures, ran the script on all of them,and produced gifs from those.
I suspect that the performance on those photos is slightly higher than expected since those photos were higher resolution than usual (at least from my own dataset and more specifically validation set and test set).

![]({{ site.url }}/assets/img/classification_animation3.gif)
![]({{ site.url }}/assets/img/classification_animation5.gif)
![]({{ site.url }}/assets/img/classification_animation6.gif)
![]({{ site.url }}/assets/img/classification_animation7.gif)
![]({{ site.url }}/assets/img/classification_animation8.gif)
![]({{ site.url }}/assets/img/classification_animation9.gif)
![]({{ site.url }}/assets/img/classification_animation10.gif)
![]({{ site.url }}/assets/img/classification_animation11.gif)
![]({{ site.url }}/assets/img/classification_animation12.gif)
![]({{ site.url }}/assets/img/classification_animation13.gif)
![]({{ site.url }}/assets/img/classification_animation14.gif)
![]({{ site.url }}/assets/img/classification_animation15.gif)
![]({{ site.url }}/assets/img/classification_animation16.gif)

